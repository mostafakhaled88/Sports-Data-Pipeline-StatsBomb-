# âš½ Sports Data Pipeline (StatsBomb)

An **end-to-end data engineering project** that ingests open football (soccer) data from **StatsBomb**, stores it in **Google Cloud Storage**, processes it using **Apache Beam / Dataflow**, and loads analytics-ready tables into **BigQuery**.

This project is designed to reflect **real-world, production-style data pipelines** with clear separation of concerns, fault tolerance, and scalability.

---

## ğŸš€ Project Goals

* Build a **cloud-native data pipeline** using GCP tools
* Apply **Bronze â†’ Silver** data architecture principles
* Practice real-world ingestion, parsing, and error handling
* Create a **portfolio-grade project** suitable for data engineering roles

---

## ğŸ§± Architecture Overview

```
StatsBomb Open Data (GitHub)
            â†“
      Ingestion Layer
  (Python + Requests)
            â†“
 Google Cloud Storage (Raw / Bronze)
            â†“
 Apache Beam / Dataflow
 (Parsing, Validation, Enrichment)
            â†“
      BigQuery Tables
   (Analytics-Ready / Silver)
```

---

## ğŸ“ Repository Structure

```
sports-data-pipeline/
â”œâ”€â”€ ingestion/          # Raw data ingestion (Bronze layer)
â”œâ”€â”€ dataflow/           # Apache Beam pipelines (Processing layer)
â”‚   â”œâ”€â”€ pipelines/      # End-to-end Beam pipelines
â”‚   â”œâ”€â”€ transforms/     # Parsing & transformation logic (DoFn)
â”‚   â”œâ”€â”€ schemas/        # BigQuery table schemas
â”‚   â”œâ”€â”€ utils/          # Shared utilities
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/               # Local helper data (CSVs, configs)
â”œâ”€â”€ setup.py            # Beam dependency packaging
â””â”€â”€ README.md           # Project documentation
```

---

## ğŸŸ¤ Ingestion Layer (Bronze)

**Location:** `ingestion/`

Responsibilities:

* Fetch raw JSON data from StatsBomb Open Data endpoints
* Store data *unchanged* in Google Cloud Storage
* Apply partitioned directory structure
* Add ingestion-time metadata

Data fetched:

* Competitions
* Matches
* Match events

ğŸ‘‰ See `ingestion/README.md` for details.

---

## âš™ï¸ Processing Layer (Apache Beam / Dataflow)

**Location:** `dataflow/`

Responsibilities:

* Read raw JSON files from GCS
* Parse and normalize nested JSON
* Enforce schemas
* Capture parsing & insertion errors
* Load clean data into BigQuery

Pipelines included:

* `competitions_pipeline.py`
* `matches_pipeline.py`
* `events_pipeline.py`

Advanced features:

* Multi-output `ParDo`
* Dead-letter tables
* BigQuery insert failure capture
* Beam Metrics for observability

ğŸ‘‰ See `dataflow/README.md` and subfolder READMEs for details.

---

## ğŸ§ª Data Quality & Reliability

This project follows **production-grade reliability patterns**:

* Raw data is never mutated
* Errors are routed to **dead-letter tables**, not dropped
* Pipelines are append-only and idempotent
* Explicit schemas prevent silent failures

---

## ğŸ“Š BigQuery Output Tables

Dataset: `statsbomb_raw`

Tables:

* `competitions`
* `matches`
* `events`
* `lineups`
* `events_deadletter`
* `bq_insert_errors`

Tables are designed for:

* Analytics
* Dashboarding
* Downstream feature engineering

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Ingest Raw Data

```bash
python ingestion/fetch_competitions.py
python ingestion/fetch_matches.py
python ingestion/fetch_events.py --matches_csv data/matches.csv
```

---

### 2ï¸âƒ£ Run Pipelines Locally (DirectRunner)

```bash
python dataflow/pipelines/competitions_pipeline.py
python dataflow/pipelines/matches_pipeline.py
```

---

### 3ï¸âƒ£ Run on Google Dataflow (Example)

```bash
python dataflow/pipelines/events_pipeline.py \
  --runner DataflowRunner \
  --project football-analytics-project \
  --region us-central1 \
  --temp_location gs://statsbomb-raw/temp \
  --staging_location gs://statsbomb-raw/staging \
  --input_path gs://statsbomb-raw/events/**/*.json \
  --output_table football-analytics-project:statsbomb_raw.events \
  --lineup_table football-analytics-project:statsbomb_raw.lineups \
  --deadletter_table football-analytics-project:statsbomb_raw.events_deadletter \
  --bq_errors_table football-analytics-project:statsbomb_raw.bq_insert_errors
```

---

## ğŸ› ï¸ Tech Stack

* **Python 3**
* **Apache Beam**
* **Google Cloud Dataflow**
* **Google Cloud Storage**
* **BigQuery**
* **StatsBomb Open Data**

---

## ğŸ¯ Skills Demonstrated

* Cloud data engineering (GCP)
* Batch data ingestion & processing
* Schema-first design
* Fault-tolerant pipelines
* Production-ready folder structure
* Sports analytics data modeling

---

## ğŸ”® Future Improvements

* Partitioned & clustered BigQuery tables
* Automated scheduling (Cloud Composer / Airflow)
* CI/CD for pipelines
* Unit & integration tests
* Gold-layer analytics marts

---

## ğŸ‘¤ Author

**Mostafa Khaled Farag**
Data Analyst / Aspiring Data Engineer
ğŸ“ Cairo, Egypt
ğŸ“§ [mosta.mk@gmail.com](mailto:mosta.mk@gmail.com)
ğŸ”— [LinkedIn](https://www.linkedin.com/in/mostafa-khaled-442b841b4/)
ğŸ’» [GitHub](https://github.com/mostafakhaled88)

---

â­ If you found this project useful or inspiring, feel free to star the repo!
